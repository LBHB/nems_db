{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacob\\AppData\\Local\\Temp\\ipykernel_10656\\3239220293.py:143: DeprecationWarning: Please use `convolve1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import convolve1d\n"
     ]
    }
   ],
   "source": [
    "# FIR test\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from nems.layers import FIR\n",
    "\n",
    "\n",
    "rank = 2\n",
    "n_outputs = 50\n",
    "filter_width = 25\n",
    "input_width = 1000\n",
    "batch_size = 10\n",
    "input_size = batch_size*input_width*rank*n_outputs\n",
    "\n",
    "# This gives an array where a[0, :15, 0, 0] = [11,...11]   # first channel, first output, 15 time points\n",
    "#                           a[0, :15, 1, 0] = [21,...21]   # second channel, first output\n",
    "#                           a[0, :15, 0, 1] = [12,...12]   # first channel, second output, etc.\n",
    "rank_number = (np.arange(rank)[..., np.newaxis] + 1)*10\n",
    "channel_number = (np.arange(n_outputs))[np.newaxis,...] + 1\n",
    "rank_and_channel = (channel_number + rank_number)[np.newaxis,...]\n",
    "with_time = rank_and_channel.repeat(input_width, axis=0)[np.newaxis,...]\n",
    "# TODO: test with batches as hundreds place?\n",
    "input_array = with_time.repeat(batch_size, axis=0)  # all batches the same for now\n",
    "# input_array = np.ones((batch_size, input_width, rank, n_outputs)).astype(float)\n",
    "# input_array = np.random.rand(batch_size, input_width, rank, n_outputs)\n",
    "input_from_wc = tf.constant(input_array.astype(float))\n",
    "\n",
    "# Passthrough kernel. Channels within rank should be summed, so for example\n",
    "# with rank 2, first output should be all (11+21=32), second (12+22=34), etc\n",
    "fir = FIR(shape=(filter_width, rank, n_outputs))\n",
    "#fir.sample_from_priors()  # test random\n",
    "#coefs_array = fir.coefficients\n",
    "coefs_array = np.zeros((filter_width, rank, n_outputs))\n",
    "coefs_array[0, :, :] = 1\n",
    "coefs_array[1, :, :] = -0.5\n",
    "fir.set_parameter_values(coefficients=coefs_array)\n",
    "coefficients = fir._reshape_coefficients().astype(float)\n",
    "coefs = tf.constant(coefficients)\n",
    "\n",
    "\n",
    "# TODO: should be able to do scipy version the same way, with np.apply or similar.\n",
    "#       Using GPU doesn't speed this one up for some reason, but can add a hook\n",
    "#       in as_tf_layer to use the other call if no gpu.\n",
    "# TODO: this is still quite slow. A lot of people reporting issues have said they\n",
    "#       get better performance by just converting to numpy and back within call()\n",
    "# TODO: how hard to switch to N-D convolution for TensorFlow? (I saw they have\n",
    "#       an equivalent fn but need to see what extra work is involved if any)\n",
    "@tf.function\n",
    "def my_func1():\n",
    "    new_coefs = tf.expand_dims(tf.transpose(coefs, [2, 0, 1]), -1)  # outputs, width, rank, 1\n",
    "    reversed_coefs = tf.reverse(new_coefs, axis=[1])  # flip time axis\n",
    "    padded_input = tf.pad(input_from_wc, [[0,0], [filter_width-1,0], [0,0], [0,0]])\n",
    "    x = tf.transpose(padded_input, [3, 0, 1, 2])  # outputs, batch, width, rank\n",
    "    y = tf.map_fn(\n",
    "        fn = lambda t: tf.nn.conv1d(t[0], t[1], stride=1, padding='VALID'),\n",
    "        elems = (x, reversed_coefs),\n",
    "        fn_output_signature=tf.float64\n",
    "        )\n",
    "    z = tf.transpose(tf.squeeze(y, axis=3), [1, 2, 0])  # batch, width, outputs\n",
    "    return z\n",
    "\n",
    "# old CPU version is much slower\n",
    "# Can't use tf.function optimization b/c of list append\n",
    "def my_func2():\n",
    "    transposed = tf.transpose(input_from_wc, [0, 1, 3, 2])  # groups by output before rank w/o transpose\n",
    "    reshaped = tf.reshape(transposed, [batch_size, input_width, rank*n_outputs])\n",
    "    padded_input = tf.pad(reshaped, [[0, 0], [filter_width-1, 0], [0, 0]])\n",
    "    reversed_coefs = reversed_coefs = tf.reverse(coefs, axis=[0])\n",
    "    L = []\n",
    "    for i in range(reshaped.shape[2]):\n",
    "        W = reversed_coefs.shape[1]\n",
    "        A = padded_input[:, :, (i*W):((i+1)*W)]\n",
    "        B = reversed_coefs[:, :, i:(i+1)]\n",
    "        L.append(tf.nn.conv1d(A, B, stride=1, padding='VALID'))\n",
    "    Y = tf.concat(L, axis=2)\n",
    "    return Y\n",
    "\n",
    "# Grouped convolutions are still much faster (on GPU), but can't run on CPU\n",
    "# Also, this seems to be about the same speed for larger output and batch size.\n",
    "# I.e. for the sizes we're working with, most of the %timeit clock is probably\n",
    "# just the overhead for porting to GPU (so the speed difference should be even\n",
    "# bigger during optimization)\n",
    "@tf.function\n",
    "def my_func3():\n",
    "    transposed = tf.transpose(input_from_wc, [0, 1, 3, 2])  # groups by output before rank w/o transpose\n",
    "    reshaped = tf.reshape(transposed, [batch_size, input_width, rank*n_outputs])\n",
    "    reversed_coefs = tf.reverse(coefs, axis=[0])\n",
    "    padded_input = tf.pad(reshaped, [[0, 0], [filter_width-1, 0], [0, 0]])\n",
    "    y = tf.nn.conv1d(padded_input, reversed_coefs, stride=1, padding='VALID')\n",
    "    return y\n",
    "\n",
    "\n",
    "# Scipy versions ####\n",
    "\n",
    "input = input_array[0,...]  # don't include batches\n",
    "padding = fir._get_filter_padding()\n",
    "#input_with_padding = np.concatenate([input, padding])  # switched?? why? TODO, compare to old NEMS again\n",
    "input_with_padding = np.concatenate([padding, input])\n",
    "\n",
    "# nditer\n",
    "def fir_1():\n",
    "    outputs = []\n",
    "    iterator = np.nditer(np.moveaxis(input_with_padding, 2, 0), flags=['external_loop'], order='C')\n",
    "    for i, x in enumerate(iterator):\n",
    "        # Have to reshape b/c iterator always returns 1d slices\n",
    "        y = scipy.signal.convolve(\n",
    "            x.reshape(input_width + filter_width-1, rank),\n",
    "            coefficients[...,i], mode='valid'\n",
    "            )\n",
    "        outputs.append(y[..., np.newaxis])\n",
    "    output = np.concatenate(outputs, axis=2)\n",
    "    # TODO: This just squeezes out rank, but also need to squeeze out\n",
    "    #       any extra dims for higher-D data.\n",
    "    return np.squeeze(output, axis=1)\n",
    "\n",
    "# double indexing\n",
    "# This is faster, oddly enough, and not just for me: this seems to be the\n",
    "# consensus from several stackoverflow posts. Faster than nditer, faster than\n",
    "# apply_over_axes, etc, and I'm not sure how else to implement this. The\n",
    "# grouped convolutions happening in tensorflow are implemented at the C level\n",
    "# in the cuDNN library so I wouldn't even know where to begin to try to\n",
    "# reimplement that here (and that's definitely not a priority).\n",
    "def fir_2():\n",
    "    outputs = []\n",
    "    for i in range(coefficients.shape[-1]):\n",
    "        y = scipy.signal.convolve(\n",
    "            input_with_padding[...,i], coefficients[...,i], mode='valid'\n",
    "            )\n",
    "        outputs.append(y[..., np.newaxis])\n",
    "    output = np.concatenate(outputs, axis=2)\n",
    "    return np.squeeze(output, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "from itertools import chain, repeat\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage.filters import convolve1d\n",
    "\n",
    "# Compare against old FIR\n",
    "\n",
    "def get_zi(b, x):\n",
    "    # This is the approach NARF uses. If the initial value of x[0] is 1,\n",
    "    # this is identical to the NEMS approach. We need to provide zi to\n",
    "    # lfilter to force it to return the final coefficients of the dummy\n",
    "    # filter operation.\n",
    "    n_taps = len(b)\n",
    "    #null_data = np.full(n_taps*2, x[0])\n",
    "    null_data = np.full(n_taps*2, 0)\n",
    "    zi = np.ones(n_taps-1)\n",
    "    return scipy.signal.lfilter(b, [1], null_data, zi=zi)[1]\n",
    "\n",
    "\n",
    "def _insert_zeros(coefficients, rate=1):\n",
    "    if rate<=1:\n",
    "        return coefficients\n",
    "\n",
    "    d1 = int(np.ceil((rate-1)/2))\n",
    "    d0 = int(rate-1-d1)\n",
    "    s = coefficients.shape\n",
    "    new_c = np.concatenate((np.zeros((s[0],s[1],d0)),\n",
    "                            np.expand_dims(coefficients, axis=2),\n",
    "                            np.zeros((s[0],s[1],d1))), axis=2)\n",
    "    new_c = np.reshape(new_c, (s[0],s[1]*rate))\n",
    "    return new_c\n",
    "\n",
    "\n",
    "def per_channel(x, coefficients, bank_count=1, non_causal=0, rate=1,\n",
    "                cross_channels=False):\n",
    "    '''Private function used by fir_filter().\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array (n_channels, n_times) or (n_channels * bank_count, n_times)\n",
    "        Input data. Can be sized two different ways:\n",
    "        option 1: number of input channels is same as total channels in the\n",
    "          filterbank, allowing a different stimulus into each filter\n",
    "        option 2: number of input channels is same as number of coefficients\n",
    "          in each fir filter, so that the same stimulus goes into each\n",
    "          filter\n",
    "    coefficients : array (n_channels * bank_count, n_taps)\n",
    "        Filter coefficients. For ``x`` option 2, input channels are nested in\n",
    "        output channel, i.e., filter ``filter_i`` of bank ``bank_i`` is at\n",
    "        ``coefficients[filter_i * n_banks + bank_i]``.\n",
    "    bank_count : int\n",
    "        Number of filters in each bank.\n",
    "    Returns\n",
    "    -------\n",
    "    signal : array (bank_count, n_times)\n",
    "        Filtered signal.\n",
    "    '''\n",
    "    # Make sure the number of input channels (x) match the number FIR filters\n",
    "    # provided (we have a separate filter for each channel). The `zip` function\n",
    "    # doesn't require the iterables to be the same length.\n",
    "    n_in = len(x)\n",
    "    if rate > 1:\n",
    "        coefficients = _insert_zeros(coefficients, rate)\n",
    "        print(coefficients)\n",
    "    n_filters = len(coefficients)\n",
    "    if bank_count>0:\n",
    "        n_banks = int(n_filters / bank_count)\n",
    "    else:\n",
    "        n_banks = n_filters\n",
    "    if cross_channels:\n",
    "        # option 0: user has specified that each filter should be applied to\n",
    "        # each input channel (requires bank_count==1)\n",
    "        # TODO : integrate with core loop below instead of pasted hack\n",
    "        out = np.zeros((n_in*n_filters, x.shape[1]))\n",
    "        i_out=0\n",
    "        for i_in in range(n_in):\n",
    "            x_ = x[i_in]\n",
    "            for i_bank in range(n_filters):\n",
    "                c = coefficients[i_bank]\n",
    "                zi = get_zi(c, x_)\n",
    "                r, zf = scipy.signal.lfilter(c, [1], x_, zi=zi)\n",
    "                out[i_out] = r\n",
    "                i_out+=1\n",
    "        return out\n",
    "    elif n_filters == n_in:\n",
    "        # option 1: number of input channels is same as total channels in the\n",
    "        # filterbank, allowing a different stimulus into each filter\n",
    "        all_x = iter(x)\n",
    "    elif n_filters == n_in * bank_count:\n",
    "        # option 2: number of input channels is same as number of coefficients\n",
    "        # in each fir filter, so that the same stimulus goes into each\n",
    "        # filter\n",
    "        one_x = tuple(x)\n",
    "        all_x = chain.from_iterable([one_x for _ in range(bank_count)])\n",
    "    else:\n",
    "        if bank_count == 1:\n",
    "            desc = '%i FIR filters' % n_filters\n",
    "        else:\n",
    "            desc = '%i FIR filter banks' % n_banks\n",
    "        raise ValueError(\n",
    "            'Dimension mismatch. %s channels provided for %s.' % (n_in, desc))\n",
    "\n",
    "    c_iter = iter(coefficients)\n",
    "    out = np.zeros((bank_count, x.shape[1]))\n",
    "    for i_out in range(bank_count):\n",
    "        for i_bank in range(n_banks):\n",
    "            x_ = next(all_x)\n",
    "            c = next(c_iter)\n",
    "            if non_causal:\n",
    "                # reverse model (using future values of input to predict)\n",
    "                x_ = np.roll(x_, -non_causal)\n",
    "\n",
    "            # It is slightly more \"correct\" to use lfilter than convolve at\n",
    "            # edges, but but also about 25% slower (Measured on Intel Python\n",
    "            # Dist, using i5-4300M)\n",
    "            zi = get_zi(c, x_)\n",
    "            r, zf = scipy.signal.lfilter(c, [1], x_, zi=zi)\n",
    "            out[i_out] += r\n",
    "    return out\n",
    "\n",
    "def old_fir():\n",
    "    # merge rank and outputs\n",
    "    old_input = input.swapaxes(1, 2).reshape(input_width, rank*n_outputs).swapaxes(0,1)\n",
    "    old_c = coefs_array.swapaxes(0,1)\n",
    "    if old_c.ndim == 3:\n",
    "        d1, d2, d3 = old_c.shape\n",
    "        old_c = np.concatenate(np.split(old_c, d3, axis=2), axis=0).squeeze()\n",
    "        banks = d3\n",
    "    return per_channel(old_input, old_c, bank_count=banks).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# All TF equiv, all scipy equiv\n",
    "print(np.all(my_func1() == my_func2()))\n",
    "print(np.all(my_func1() == my_func3()))\n",
    "print(np.all(fir_1() == fir_2()))\n",
    "# TF equiv to scipy (ignoring batches, and some rounding error)\n",
    "# TODO: had to switch padding from prepend to append for scipy to get this\n",
    "#       to match, why? what changed from before?\n",
    "# Answer: Had to flip time dimension of coefs in TF function. Something weird with TF conv1d?\n",
    "print(np.all(my_func1().numpy()[0,...].round(10) == fir_2().round(10)))\n",
    "print(np.all(my_func3().numpy()[0,...].round(10) == old_fir().round(10)))\n",
    "print(np.all(fir_2().round(8) == old_fir().round(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_fir()[:100, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0., -16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,  16.,\n",
       "        16.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func3().numpy()[0,:100,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16.])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_fir()[:100, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=float64, numpy=\n",
       "array([16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16.])>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func3()[0, :100, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d9bbb7efa0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# ax1.imshow(my_func3().numpy()[0,:,:3].T, aspect='auto', interpolation='none')\n",
    "# ax2.imshow(old_fir()[:,:3].T, aspect='auto', interpolation='none')\n",
    "ax1.imshow(coefs.numpy()[:,:,0].T, aspect='auto')\n",
    "ax2.imshow(coefficients[:,:,0].T, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.3 ms ± 4.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "129 ms ± 12.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/device:CPU:0\"):\n",
    "    my_func1() # compile\n",
    "    my_func2()\n",
    "    %timeit -n 10 my_func1()  # new CPU\n",
    "    %timeit -n 10 my_func2()  # old CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms ± 13.9 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.88 ms ± 381 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    my_func1()\n",
    "    my_func2()\n",
    "    %timeit -n 10 my_func1()         # new GPU\n",
    "    %timeit -n 10 my_func3()         # old GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.6 ms ± 3.36 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "19.9 ms ± 3.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fir_1()\n",
    "%timeit fir_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Needs lower Numpy version\n",
    "#       Also possibly incompatible with TF? Noticed it installs\n",
    "#       MKL, which I was having issues with. Should test with\n",
    "#       a separate environment\n",
    "\n",
    "# with numba\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def num_fir1():\n",
    "    fir_1()\n",
    "\n",
    "@jit(nopython=True)\n",
    "def num_fir2():\n",
    "    fir_2()\n",
    "\n",
    "# compile\n",
    "num_fir1()\n",
    "num_fir2()\n",
    "%timeit -n 100 num_fir1()\n",
    "%timeit -n 100 num_fir2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WC test\n",
    "# Comment out to use same as FIR\n",
    "# rank = 4\n",
    "# n_outputs = 12\n",
    "# input_width = 1000\n",
    "# batch_size = 1\n",
    "# input_size = batch_size*input_width*rank*n_outputs\n",
    "# WC only\n",
    "input_channels = 18\n",
    "\n",
    "# Input is all ones for first channel, all twos for second channel, etc.\n",
    "# (same for all batches)\n",
    "# channel_number = (np.arange(input_channels))[np.newaxis,...] + 1\n",
    "# with_time = channel_number.repeat(input_width, axis=0)[np.newaxis,...]\n",
    "# input_array = with_time.repeat(batch_size, axis=0)\n",
    "# inputs = tf.constant(input_array.astype(float))\n",
    "inputs = tf.constant(np.random.rand(batch_size, input_width, input_channels))\n",
    "\n",
    "# 1/input_channels for all channels, so output should be sum of channel numbers\n",
    "#coefs = np.ones((input_channels, rank, n_outputs))\n",
    "#output_should_be = sum(range(1, input_channels+1))\n",
    "coefs = np.random.rand(input_channels, rank, n_outputs)\n",
    "\n",
    "@tf.function\n",
    "def my_func4():\n",
    "    # new version (tensordot)\n",
    "    out = tf.tensordot(inputs, coefs, axes=[[2], [0]])\n",
    "    return out\n",
    "    # return tf.reshape(out, [batch_size, input_width, -1])\n",
    "\n",
    "@tf.function\n",
    "def my_func5():\n",
    "    # old version (convolution)\n",
    "    transposed = tf.transpose(coefs, [0, 2, 1])\n",
    "    reshaped = tf.reshape(coefs, [input_channels, rank*n_outputs])\n",
    "    return tf.nn.conv1d(\n",
    "        inputs, tf.expand_dims(reshaped, 0), stride=1,\n",
    "        padding='SAME'\n",
    "        )\n",
    "\n",
    "\n",
    "def wc_1():\n",
    "    return np.moveaxis(inputs.numpy()[0,...] @ np.moveaxis(coefs, -1, 0), 0, -1)\n",
    "\n",
    "def wc_2():\n",
    "    return np.tensordot(inputs.numpy()[0,...], coefs, axes=(1, 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 1000, 50])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_func5().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# TF functions equivalent\n",
    "print(np.all(my_func4() == tf.reshape(my_func5(), [batch_size, input_width, rank, n_outputs])))\n",
    "\n",
    "# scipy equivalent to each other within decent precision\n",
    "print(np.all(wc_1().round(10) == wc_2().round(10)))\n",
    "\n",
    "# TF equivalent to scipy\n",
    "print(np.all(my_func4().numpy()[0,...].round(10) == wc_1().round(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.07 ms ± 964 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "7.99 ms ± 738 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 with tf.device(\"/device:CPU:0\"): my_func4()  # new CPU\n",
    "%timeit -n 10 with tf.device(\"/device:CPU:0\"): my_func5()  # old CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 ms ± 506 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.55 ms ± 508 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 with tf.device(\"/GPU:0\"): my_func4()         # new GPU\n",
    "%timeit -n 10 with tf.device(\"/GPU:0\"): my_func5()         # old GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.03 ms ± 502 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "1.67 ms ± 340 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Scipy version. Why is this so much faster? Overhead with setting up\n",
    "# TF objects maybe?\n",
    "# Ah... Still much faster than TF-CPU, but GPU scales better. So yes, probably\n",
    "# a lot of the TF time is overhead that wouldn't be important during fitting.\n",
    "%timeit wc_1()\n",
    "%timeit wc_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does moving axes on the coefs add much?\n",
    "# Answer: yeah some, it's ~25% slower with those ops. But probably not enough\n",
    "# to make a huge difference.\n",
    "coefs2 = np.moveaxis(coefs, -1, 0)\n",
    "%timeit np.moveaxis(inputs.numpy()[0,...] @ coefs2, 0, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('mkltest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e04c614ea49137b728f9a88f47ebc62abd1dc770924cb0e56431acc2e9f8803"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
