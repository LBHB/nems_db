#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun 14 09:33:47 2017
@author: svd, changes added by njs
"""

import logging
import re
import os
import os.path
import scipy.io
import scipy.ndimage.filters
import scipy.signal
import numpy as np
import json
import sys
import io
import datetime
import glob
from math import isclose
import copy

import pandas as pd
import matplotlib.pyplot as plt
import nems.signal
import nems.recording
import nems_db.db as db
from nems.recording import Recording
from nems.recording import load_recording


# TODO: Replace catch-all `except:` statements with except SpecificError,
#       or add some other way to help with debugging them.

# paths to baphy data -- standard locations on elephant
stim_cache_dir = '/auto/data/tmp/tstim/'  # location of cached stimuli
spk_subdir = 'sorted/'   # location of spk.mat files relative to parmfiles

# TODO: Replace print() statements with log.info()?
log = logging.getLogger(__name__)


def baphy_mat2py(s):

    s3 = re.sub(r';', r'', s.rstrip())
    s3 = re.sub(r'%', r'#', s3)
    s3 = re.sub(r'\\', r'/', s3)
    s3 = re.sub(r"\.([a-zA-Z0-9]+)'", r"XX\g<1>'", s3)
    s3 = re.sub(r"\.([a-zA-Z0-9]+)\+", r"XX\g<1>+", s3)
    s3 = re.sub(r"\.([a-zA-Z0-9]+) ,", r"XX\g<1> ,", s3)
    s3 = re.sub(r'globalparams\(1\)', r'globalparams', s3)
    s3 = re.sub(r'exptparams\(1\)', r'exptparams', s3)

    s4 = re.sub(r'\(([0-9]*)\)', r'[\g<1>]', s3)

    s5 = re.sub(r'\.([A-Za-z][A-Za-z0-9_]+)', r"['\g<1>']", s4)

    s6 = re.sub(r'([0-9]+) ', r"\g<0>,", s5)
    s6 = re.sub(r'NaN ', r"np.nan,", s6)
    s6 = re.sub(r'Inf ', r"np.inf,", s6)

    s7 = re.sub(r"XX([a-zA-Z0-9]+)'", r".\g<1>'", s6)
    s7 = re.sub(r"XX([a-zA-Z0-9]+)\+", r".\g<1>+", s7)
    s7 = re.sub(r"XX([a-zA-Z0-9]+) ,", r".\g<1> ,", s7)
    s7 = re.sub(r',,', r',', s7)
    s7 = re.sub(r',Hz', r'Hz', s7)
    s7 = re.sub(r'NaN', r'np.nan', s7)
    s7 = re.sub(r'zeros\(([0-9,]+)\)', r'np.zeros([\g<1>])', s7)
    s7 = re.sub(r'{(.*)}', r'[\g<1>]', s7)

    return s7


def baphy_parm_read(filepath):
    print("Loading {0}".format(filepath))

    f = io.open(filepath, "r")
    s = f.readlines(-1)

    globalparams = {}
    exptparams = {}
    exptevents = {}

    for ts in s:
        sout = baphy_mat2py(ts)
        # print(sout)
        try:
            exec(sout)
        except KeyError:
            ts1 = sout.split('= [')
            ts1 = ts1[0].split(',[')

            s1 = ts1[0].split('[')
            sout1 = "[".join(s1[:-1]) + ' = {}'
            try:
                exec(sout1)
            except:
                s2 = sout1.split('[')
                sout2 = "[".join(s2[:-1]) + ' = {}'
                try:
                    exec(sout2)
                except:
                    s3 = sout2.split('[')
                    sout3 = "[".join(s3[:-1]) + ' = {}'
                    exec(sout3)
                    exec(sout2)

                exec(sout1)
            exec(sout)
        except NameError:
            print("NameError on: {0}".format(sout))
        except:
            print("Other error on: {0} to {1}".format(ts,sout))

    # special conversions

    # convert exptevents to a DataFrame:
    t = [exptevents[k] for k in exptevents]
    d = pd.DataFrame(t)
    if 'ClockStartTime' in d.columns:
        exptevents = d.drop(['Rove', 'ClockStartTime'], axis=1)
    else:
        exptevents = d.drop(['Rove'], axis=1)

    # rename columns to NEMS standard epoch names
    exptevents.columns = ['name', 'start', 'end', 'Trial']
    for i in range(len(exptevents)):
        if exptevents.loc[i, 'end'] == []:
            exptevents.loc[i, 'end'] = exptevents.loc[i, 'start']

    return globalparams, exptparams, exptevents


def baphy_load_specgram(stimfilepath):

    matdata = scipy.io.loadmat(stimfilepath, chars_as_strings=True)

    stim = matdata['stim']

    stimparam = matdata['stimparam'][0][0]

    try:
        # case 1: loadstimfrombaphy format
        # remove redundant tags from tag list and stimulus array
        d = matdata['stimparam'][0][0][0][0]
        d = [x[0] for x in d]
        tags, tagids = np.unique(d, return_index=True)

        stim = stim[:, :, tagids]
    except:
        # loadstimbytrial format. don't want to filter by unique tags.
        # field names within stimparam don't seem to be preserved
        # in this load format??
        d = matdata['stimparam'][0][0][2][0]
        tags = [x[0] for x in d]

    return stim, tags, stimparam


def baphy_stim_cachefile(exptparams, parmfilepath=None, **options):
    """
    generate cache filename generated by loadstimfrombaphy

    code adapted from loadstimfrombaphy.m
    """

    if 'truncatetargets' not in options:
        options['truncatetargets'] = 1
    if 'pertrial' not in options:
        options['pertrial'] = False

    if options['pertrial']:
        # loadstimbytrial cache filename format
        pp, bb = os.path.split(parmfilepath)
        bb = bb.split(".")[0]
        dstr = "loadstimbytrial_{0}_ff{1}_fs{2}_cc{3}_trunc{4}.mat".format(
                     bb, options['stimfmt'], options['rasterfs'],
                     options['chancount'], options['truncatetargets']
                     )
        return stim_cache_dir + dstr

    # otherwise use standard load stim from baphy format
    if options['runclass'] is None:
        RefObject = exptparams['TrialObject'][1]['ReferenceHandle'][1]
    elif 'runclass' in exptparams.keys():
        runclass = exptparams['runclass'].split("_")
        if (len(runclass) > 1) and (runclass[1] == options["runclass"]):
            RefObject = exptparams['TrialObject'][1]['TargetHandle'][1]
        else:
            RefObject = exptparams['TrialObject'][1]['ReferenceHandle'][1]
    else:
        RefObject = exptparams['TrialObject'][1]['ReferenceHandle'][1]

    dstr = RefObject['descriptor']
    if dstr == 'Torc':
        if 'RunClass' in exptparams['TrialObject'][1].keys():
            dstr += '-'+exptparams['TrialObject'][1]['RunClass']
        else:
            dstr += '-TOR'

    # include all parameter values, even defaults, in filename
    fields = RefObject['UserDefinableFields']
    for cnt1 in range(0, len(fields), 3):
        if RefObject[fields[cnt1]] == 0:
            RefObject[fields[cnt1]] = int(0)
            # print(fields[cnt1])
            # print(RefObject[fields[cnt1]])
            # print(dstr)
        dstr = "{0}-{1}".format(dstr, RefObject[fields[cnt1]])

    dstr = re.sub(r":", r"", dstr)

    if 'OveralldB' in exptparams['TrialObject'][1]:
        OveralldB = exptparams['TrialObject'][1]['OveralldB']
        dstr += "-{0}dB".format(OveralldB)
    else:
        OveralldB = 0

    dstr += "-{0}-fs{1}-ch{2}".format(
            options['stimfmt'], options['rasterfs'], options['chancount']
            )

    if options['includeprestim']:
        dstr += '-incps1'

    dstr = re.sub(r"[ ,]", r"_", dstr)
    dstr = re.sub(r"[\[\]]", r"", dstr)

    return stim_cache_dir + dstr + '.mat'


def baphy_load_spike_data_raw(spkfilepath, channel=None, unit=None):

    matdata = scipy.io.loadmat(spkfilepath, chars_as_strings=True)

    sortinfo = matdata['sortinfo']
    if sortinfo.shape[0] > 1:
        sortinfo = sortinfo.T
    sortinfo = sortinfo[0]

    # figure out sampling rate, used to convert spike times into seconds
    spikefs = matdata['rate'][0][0]

    return sortinfo, spikefs


def baphy_align_time(exptevents, sortinfo, spikefs, finalfs=0):

    # number of channels in recording (not all necessarily contain spikes)
    chancount = len(sortinfo)

    # figure out how long each trial is by the time of the last spike count.
    # this method is a hack!
    # but since recordings are longer than the "official"
    # trial end time reported by baphy, this method preserves extra spikes
    TrialCount = np.max(exptevents['Trial'])
    TrialLen_sec = np.array(
            exptevents.loc[exptevents['name'] == "TRIALSTOP"]['start']
            )
    TrialLen_spikefs = np.concatenate(
            (np.zeros([1, 1]), TrialLen_sec[:, np.newaxis]*spikefs), axis=0
            )

    for c in range(0, chancount):
        if len(sortinfo[c]) and sortinfo[c][0].size:
            s = sortinfo[c][0][0]['unitSpikes']
            s = np.reshape(s, (-1, 1))
            unitcount = s.shape[0]
            for u in range(0, unitcount):
                st = s[u, 0]

                # print('chan {0} unit {1}: {2} spikes'.format(c,u,st.shape[1]))
                for trialidx in range(1, TrialCount+1):
                    ff = (st[0, :] == trialidx)
                    if np.sum(ff):
                        utrial_spikefs = np.max(st[1, ff])
                        TrialLen_spikefs[trialidx, 0] = np.max(
                                [utrial_spikefs, TrialLen_spikefs[trialidx, 0]]
                                )

    # using the trial lengths, figure out adjustments to trial event times.
    if finalfs:
        print('rounding Trial offset spike times'
              ' to even number of rasterfs bins')
        # print(TrialLen_spikefs)
        TrialLen_spikefs = (
                np.ceil(TrialLen_spikefs / spikefs*finalfs)
                / finalfs*spikefs
                )
        # print(TrialLen_spikefs)

    Offset_spikefs = np.cumsum(TrialLen_spikefs)
    Offset_sec = Offset_spikefs / spikefs  # how much to offset each trial

    # adjust times in exptevents to approximate time since experiment started
    # rather than time since trial started (native format)
    for Trialidx in range(1, TrialCount+1):
        # print("Adjusting trial {0} by {1} sec"
        #       .format(Trialidx,Offset_sec[Trialidx-1]))
        ff = (exptevents['Trial'] == Trialidx)
        exptevents.loc[ff, ['start', 'end']] = (
                exptevents.loc[ff, ['start', 'end']] + Offset_sec[Trialidx-1]
                )

        # ff = ((exptevents['Trial'] == Trialidx)
        #       & (exptevents['end'] > Offset_sec[Trialidx]))
        # badevents, = np.where(ff)
        # print("{0} events past end of trial?".format(len(badevents)))
        # exptevents.drop(badevents)

    print("{0} trials totaling {1:.2f} sec".format(TrialCount, Offset_sec[-1]))

    # convert spike times from samples since trial started to
    # (approximate) seconds since experiment started (matched to exptevents)
    totalunits = 0
    spiketimes = []  # list of spike event times for each unit in recording
    unit_names = []  # string suffix for each unit (CC-U)
    chan_names = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']
    for c in range(0, chancount):
        if len(sortinfo[c]) and sortinfo[c][0].size:
            s = sortinfo[c][0][0]['unitSpikes']
            comment = sortinfo[c][0][0][0][0][2][0]
            log.info('Comment: %s', comment)

            s = np.reshape(s, (-1, 1))
            unitcount = s.shape[0]
            for u in range(0, unitcount):
                st = s[u, 0]
                uniquetrials = np.unique(st[0, :])
                # print('chan {0} unit {1}: {2} spikes {3} trials'
                #       .format(c, u, st.shape[1], len(uniquetrials)))

                unit_spike_events = np.array([])
                for trialidx in uniquetrials:
                    ff = (st[0, :] == trialidx)
                    this_spike_events = (st[1, ff]
                                         + Offset_spikefs[np.int(trialidx-1)])
                    if comment == 'PC-cluster sorted by mespca.m':
                        # remove last spike, which is stray
                        this_spike_events = this_spike_events[:-1]
                    unit_spike_events = np.concatenate(
                            (unit_spike_events, this_spike_events), axis=0
                            )
                    # print("   trial {0} first spike bin {1}"
                    #       .format(trialidx,st[1,ff]))

                totalunits += 1
                if chancount <= 8:
                    unit_names.append("{0}{1}".format(chan_names[c], u+1))
                else:
                    unit_names.append("{0:02d}-{1}".format(c+1, u+1))
                spiketimes.append(unit_spike_events / spikefs)

    return exptevents, spiketimes, unit_names


def baphy_load_pupil_trace(pupilfilepath, exptevents, **options):
    """
    returns big_rs which is pupil trace resampled to options['rasterfs']
    and strialidx, which is the index into big_rs for the start of each
    trial. need to make sure the big_rs vector aligns with the other signals
    """

    rasterfs = options.get('rasterfs', 1000)
    pupil_offset = options.get('pupil_offset', 0.75)
    pupil_deblink = options.get('pupil_deblink', True)
    pupil_median = options.get('pupil_median', 0)
    pupil_smooth = options.get('pupil_smooth', 0)
    pupil_highpass = options.get('pupil_highpass', 0)
    pupil_lowpass = options.get('pupil_lowpass', 0)
    pupil_bandpass = options.get('pupil_bandpass', 0)
    pupil_derivative = options.get('pupil_derivative', '')
    pupil_mm = options.get('pupil_mm', False)
    verbose = options.get('verbose', False)

    if pupil_smooth:
        raise ValueError('pupil_smooth not implemented. try pupil_median?')
    if pupil_highpass:
        raise ValueError('pupil_highpass not implemented.')
    if pupil_lowpass:
        raise ValueError('pupil_lowpass not implemented.')
    if pupil_bandpass:
        raise ValueError('pupil_bandpass not implemented.')
    if pupil_derivative:
        raise ValueError('pupil_derivative not implemented.')
    if pupil_mm:
        raise ValueError('pupil_mm not implemented.')

    matdata = scipy.io.loadmat(pupilfilepath)

    p = matdata['pupil_data']
    params = p['params']
    if 'pupil_variable_name' not in options:
        options['pupil_variable_name'] = params[0][0]['default_var'][0][0][0]
        print("Using default pupil_variable_name: " +
              options['pupil_variable_name'])
    if 'pupil_algorithm' not in options:
        options['pupil_algorithm'] = params[0][0]['default'][0][0][0]
        print("Using default pupil_algorithm: " + options['pupil_algorithm'])

    results = p['results'][0][0][-1][options['pupil_algorithm']]
    pupil_diameter = np.array(results[0][options['pupil_variable_name']][0][0])
    if pupil_diameter.shape[0] == 1:
        pupil_diameter = pupil_diameter.T
    print("pupil_diameter.shape: " + str(pupil_diameter.shape))

    fs_approximate = 10  # approx video framerate
    if pupil_deblink:
        dp = np.abs(np.diff(pupil_diameter, axis=0))
        blink = np.zeros(dp.shape)
        blink[dp > np.nanmean(dp) + 6*np.nanstd(dp)] = 1
        # CRH add following line 7-19-2019
        # (blink should be = 1 if pupil_dia goes to 0)
        blink[[isclose(p, 0, abs_tol=0.5) for p in pupil_diameter[:-1]]] = 1
        box = np.ones([fs_approximate]) / (fs_approximate)
        # print(blink.shape)
        blink = np.convolve(blink[:, 0], box, mode='same')
        blink[blink > 0] = 1
        blink[blink <= 0] = 0
        onidx, = np.where(np.diff(blink) > 0)
        offidx, = np.where(np.diff(blink) < 0)

        if onidx[0] > offidx[0]:
            onidx = np.concatenate((np.array([0]), onidx))
        if len(onidx) > len(offidx):
            offidx = np.concatenate((offidx, np.array([len(blink)])))
        deblinked = pupil_diameter.copy()
        for i, x1 in enumerate(onidx):
            x2 = offidx[i]
            if x2 < x1:
                print([i, x1, x2])
                print("WHAT'S UP??")
            else:
                # print([i,x1,x2])
                deblinked[x1:x2, 0] = np.linspace(
                        deblinked[x1], deblinked[x2-1], x2-x1
                        )

        if verbose:
            plt.figure()
            plt.plot(pupil_diameter)
            plt.plot(deblinked)
            plt.xlabel('Frame')
            plt.ylabel('Pupil')
            plt.legend('Raw', 'Deblinked')
            plt.title("Artifacts detected: {}".format(len(onidx)))
        pupil_diameter = deblinked

    # resample and remove dropped frames

    # find and parse pupil events
    pp = ['PUPIL,' in x['name'] for i, x in exptevents.iterrows()]
    trials = list(exptevents.loc[pp, 'Trial'])
    ntrials = len(trials)
    timestamp = np.zeros([ntrials+1])
    firstframe = np.zeros([ntrials+1])
    for i, x in exptevents.loc[pp].iterrows():
        t = x['Trial'] - 1
        s = x['name'].split(",[")
        p = eval("["+s[1])
        # print("{0} p=[{1}".format(i,s[1]))
        timestamp[t] = p[0]
        firstframe[t] = int(p[1])
    pp = ['PUPILSTOP' in x['name'] for i, x in exptevents.iterrows()]
    lastidx = np.argwhere(pp)[-1]

    s = exptevents.iloc[lastidx[0]]['name'].split(",[")
    p = eval("[" + s[1])
    timestamp[-1] = p[0]
    firstframe[-1] = int(p[1])

    # align pupil with other events, probably by
    # removing extra bins from between trials
    ff = exptevents['name'].str.startswith('TRIALSTART')
    start_events = exptevents.loc[ff, ['start']].reset_index()
    start_events['StartBin'] = (
            np.round(start_events['start'] * options['rasterfs'])
            ).astype(int)
    start_e = list(start_events['StartBin'])
    ff = (exptevents['name'] == 'TRIALSTOP')
    stop_events = exptevents.loc[ff, ['start']].reset_index()
    stop_events['StopBin'] = (
            np.round(stop_events['start'] * options['rasterfs'])
            ).astype(int)
    stop_e = list(stop_events['StopBin'])

    # calculate frame count and duration of each trial
    duration = np.diff(timestamp) * 24*60*60
    frame_count = np.diff(firstframe)

    # warp/resample each trial to compensate for dropped frames
    strialidx = np.zeros([ntrials + 1])
    big_rs = np.array([])

    for ii in range(0, ntrials):
        d = pupil_diameter[
                int(firstframe[ii]):int(firstframe[ii]+frame_count[ii]), 0
                ]
        fs = frame_count[ii] / duration[ii]
        t = np.arange(0, len(d)) / fs
        ti = np.arange(
                (1/rasterfs)/2, duration[ii]+(1/rasterfs)/2, 1/rasterfs
                )
        # print("{0} len(d)={1} len(ti)={2} fs={3}"
        #       .format(ii,len(d),len(ti),fs))
        di = np.interp(ti, t, d)
        big_rs = np.concatenate((big_rs, di), axis=0)
        if (ii < ntrials-1) and (len(big_rs) > start_e[ii+1]):
            big_rs = big_rs[:start_e[ii+1]]
        elif ii == ntrials-1:
            big_rs = big_rs[:stop_e[ii]]
        strialidx[ii+1] = len(big_rs)

    if pupil_median:
        kernel_size = int(round(pupil_median*rasterfs/2)*2+1)
        big_rs = scipy.signal.medfilt(big_rs, kernel_size=kernel_size)

    # shift pupil trace by offset, usually 0.75 sec
    offset_frames = int(pupil_offset*rasterfs)
    big_rs = np.roll(big_rs, -offset_frames)
    big_rs[-offset_frames:] = np.nan

    # shape to 1 x T to match NEMS signal specs
    big_rs = big_rs[np.newaxis, :]

    return big_rs, strialidx


def baphy_load_data(parmfilepath, **options):
    """
    this feeds into baphy_load_recording and baphy_load_recording_RDT (see
        below)
    input:
        parmfilepath: baphy parameter file
        options: dictionary of loading options
            runclass: matches Reference1 or Reference2 events, depending

    current outputs:
        exptevents: pandas dataframe with one row per event. times in sec
              since experiment began
        spiketimes: list of lists. outer list indicates unit, inner list is
              the set of spike times (secs since expt started) for that unit
        unit_names: list of strings uniquely identifier each units by
              channel-unitnum (CC-U). can append to siteid- to get cellid
        stim: [channel X time X event] stimulus (spectrogram) matrix
        tags: list of string identifiers associate with each stim event
              (can be used to find events in exptevents)

    other things that could be returned:
        globalparams, exptparams: dictionaries with expt metadata from baphy

    """
    # default_options={'rasterfs':100, 'includeprestim':True,
    #                  'stimfmt':'ozgf', 'chancount':18,
    #                  'cellid': 'all'}
    # options = options.update(default_options)

    # add .m extension if missing
    if parmfilepath[-2:] != ".m":
        parmfilepath += ".m"
    # load parameter file
    globalparams, exptparams, exptevents = baphy_parm_read(parmfilepath)

    # TODO: use paths that match LBHB filesystem? new s3 filesystem?
    #       or make s3 match LBHB?

    # figure out stimulus cachefile to load
    if 'stim' in options.keys() and options['stim']:
        stimfilepath = baphy_stim_cachefile(exptparams, parmfilepath, **options)
        print("Cached stim: {0}".format(stimfilepath))
        # load stimulus spectrogram
        stim, tags, stimparam = baphy_load_specgram(stimfilepath)

        if options["stimfmt"]=='envelope' and \
            exptparams['TrialObject'][1]['ReferenceClass']=='SSA':
            # SSA special case
            stimo=stim.copy()
            maxval=np.max(np.reshape(stimo,[2,-1]),axis=1)
            print('special case for SSA stim!')
            ref=exptparams['TrialObject'][1]['ReferenceHandle'][1]
            stimlen=ref['PipDuration']+ref['PipInterval']
            stimbins=int(stimlen*options['rasterfs'])

            stim=np.zeros([2,stimbins,6])
            prebins=int(ref['PipInterval']/2*options['rasterfs'])
            durbins=int(ref['PipDuration']*options['rasterfs'])
            stim[0,prebins:(prebins+durbins),0:3]=maxval[0]
            stim[1,prebins:(prebins+durbins),3:]=maxval[1]
            tags=["{}+ONSET".format(ref['Frequencies'][0]),
                  "{}+{:.2f}".format(ref['Frequencies'][0],ref['F1Rates'][0]),
                  "{}+{:.2f}".format(ref['Frequencies'][0],ref['F1Rates'][1]),
                  "{}+ONSET".format(ref['Frequencies'][1]),
                  "{}+{:.2f}".format(ref['Frequencies'][1],ref['F1Rates'][0]),
                  "{}+{:.2f}".format(ref['Frequencies'][1],ref['F1Rates'][1])]

    else:
        stim = np.array([])

        if options['runclass'] is None:
            stim_object = 'ReferenceHandle'
        elif 'runclass' in exptparams.keys():
            runclass = exptparams['runclass'].split("_")
            if (len(runclass) > 1) and (runclass[1] == options["runclass"]):
                stim_object = 'TargetHandle'
            else:
                stim_object = 'ReferenceHandle'
        else:
            stim_object = 'ReferenceHandle'

        tags = exptparams['TrialObject'][1][stim_object][1]['Names']
        tags, tagids = np.unique(tags, return_index=True)
        stimparam = []

    # figure out spike file to load
    pp, bb = os.path.split(parmfilepath)
    spkfilepath = pp + '/' + spk_subdir + re.sub(r"\.m$", ".spk.mat", bb)
    print("Spike file: {0}".format(spkfilepath))

    # load spike times
    sortinfo, spikefs = baphy_load_spike_data_raw(spkfilepath)

    # adjust spike and event times to be in seconds since experiment started
    exptevents, spiketimes, unit_names = baphy_align_time(
            exptevents, sortinfo, spikefs, options['rasterfs']
            )

    # assign cellids to each unit
    siteid = globalparams['SiteID']
    unit_names = [(siteid + "-" + x) for x in unit_names]
    # print(unit_names)

    # test for special case where psuedo cellid suffix has been added to
    # cellid by stripping anything after a "_" underscore in the cellid (list)
    # provided
    pcellids = options['cellid'] if (type(options['cellid']) is list) \
       else [options['cellid']]
    cellids = []
    pcellidmap = {}
    for pcellid in pcellids:
        t = pcellid.split("_")
        t[0] = t[0].lower()
        cellids.append(t[0])
        pcellidmap[t[0]] = pcellid
    print(pcellidmap)
    # pull out a single cell if 'all' not specified
    spike_dict = {}
    for i, x in enumerate(unit_names):
        if (cellids[0] == 'all'):
            spike_dict[x] = spiketimes[i]
        elif (x.lower() in cellids):
            spike_dict[pcellidmap[x.lower()]] = spiketimes[i]

    if not spike_dict:
        raise ValueError('No matching cellid in baphy spike file')

    state_dict = {}
    if options['pupil']:
        try:
            pupilfilepath = re.sub(r"\.m$", ".pup.mat", parmfilepath)
            pupiltrace, ptrialidx = baphy_load_pupil_trace(
                    pupilfilepath, exptevents, **options
                    )
            state_dict['pupiltrace'] = pupiltrace

        except ValueError:
            raise ValueError("Error loading pupil data: " + pupilfilepath)

    return (exptevents, stim, spike_dict, state_dict,
            tags, stimparam, exptparams)


def baphy_load_dataset(parmfilepath, **options):
    """
    this can be used to generate a recording object

    input:
        parmfilepath: baphy parameter file
        options: dictionary of loading options

    current outputs:
        event_times: pandas dataframe with one row per event. times in sec
              since experiment began
        spike_dict: dictionary of lists. spike_dict[cellid] is the set of
              spike times (secs since expt started) for that unit
        stim_dict: stim_dict[name] is [channel X time] stimulus
              (spectrogram) matrix, the times that the stimuli were played
              are rows in the event_times dataframe

    TODO: support for pupil and behavior. branch out different functions for
        different batches of analysis. (see RDT special case below)
    other things that could be returned:
        globalparams, exptparams: dictionaries with expt metadata from baphy

    """
    # get the relatively un-pre-processed data
    exptevents, stim, spike_dict, state_dict, tags, stimparam, exptparams = \
        baphy_load_data(parmfilepath, **options)

    # pre-process event list (event_times) to only contain useful events
    # extract each trial
    print('Creating trial events')
    tag_mask_start = "TRIALSTART"
    tag_mask_stop = "TRIALSTOP"
    ffstart = exptevents['name'].str.startswith(tag_mask_start)
    ffstop = (exptevents['name'] == tag_mask_stop)
    TrialCount = np.max(exptevents.loc[ffstart, 'Trial'])
    event_times = pd.concat([exptevents.loc[ffstart, ['start']].reset_index(),
                             exptevents.loc[ffstop, ['end']].reset_index()],
                            axis=1)
    event_times['name'] = "TRIAL"
    event_times = event_times.drop(columns=['index'])

    print('Removing post-response stimuli')
    keepevents = np.full(len(exptevents), True, dtype=bool)
    for trialidx in range(1, TrialCount+1):
        # remove stimulus events after TRIALSTOP or STIM,OFF event
        fftrial_stop = (exptevents['Trial'] == trialidx) & \
            ((exptevents['name'] == "STIM,OFF") |
             (exptevents['name'] == "TRIALSTOP"))
        if np.sum(fftrial_stop):
            trialstoptime = np.min(exptevents[fftrial_stop]['start'])

            fflate = (exptevents['Trial'] == trialidx) & \
                exptevents['name'].str.startswith('Stim , ') & \
                (exptevents['start'] > trialstoptime)
            fftrunc = (exptevents['Trial'] == trialidx) & \
                exptevents['name'].str.startswith('Stim , ') & \
                (exptevents['start'] <= trialstoptime) & \
                (exptevents['end'] > trialstoptime)

        for i, d in exptevents.loc[fflate].iterrows():
            # print("{0}: {1} - {2} - {3}>{4}"
            #       .format(i, d['Trial'], d['name'], d['end'], start))
            # remove Pre- and PostStimSilence as well
            keepevents[(i-1):(i+2)] = False

        for i, d in exptevents.loc[fftrunc].iterrows():
            print("Truncating event {0} early at {1}"
                  .format(i, trialstoptime))
            exptevents.loc[i, 'end'] = trialstoptime
            # also trim post stim silence
            exptevents.loc[i + 1, 'start'] = trialstoptime
            exptevents.loc[i + 1, 'end'] = trialstoptime

    print("Keeping {0}/{1} events that precede responses"
          .format(np.sum(keepevents), len(keepevents)))
    exptevents = exptevents[keepevents].reset_index()

    # add event characterizing outcome of each behavioral
    # trial (if behavior)
    print('Creating trial outcome events')
    note_map = {'OUTCOME,FALSEALARM': 'FA_TRIAL',
                'OUTCOME,MISS': 'MISS_TRIAL',
                'BEHAVIOR,PUMPON,Pump': 'HIT_TRIAL'}
    this_event_times = event_times.copy()
    any_behavior = False
    for trialidx in range(1, TrialCount+1):
        # determine behavioral outcome, log event time to add epochs
        # spanning each trial
        ff = (((exptevents['name'] == 'OUTCOME,FALSEALARM')
              | (exptevents['name'] == 'OUTCOME,MISS')
              | (exptevents['name'] == 'BEHAVIOR,PUMPON,Pump'))
              & (exptevents['Trial'] == trialidx))

        for i, d in exptevents.loc[ff].iterrows():
            # print("{0}: {1} - {2} - {3}"
            #       .format(i, d['Trial'], d['name'], d['end']))
            this_event_times.loc[trialidx-1, 'name'] = note_map[d['name']]
            any_behavior = True

    # figure out length of entire experiment
    file_start_time = np.min(event_times['start'])
    file_stop_time = np.max(event_times['end'])
    te = pd.DataFrame(index=[0], columns=(event_times.columns))

    if any_behavior:
        # only concatenate newly labeled trials if events occured that reflect
        # behavior. There's probably a less kludgy way of checking for this
        # before actually running through the above loop
        event_times = pd.concat([event_times, this_event_times])
        te.loc[0] = [file_start_time, file_stop_time, 'ACTIVE_EXPERIMENT']
    else:
        te.loc[0] = [file_start_time, file_stop_time, 'PASSIVE_EXPERIMENT']
    event_times = event_times.append(te)

    # ADD epoch for FILENAME
    b = os.path.splitext(os.path.basename(parmfilepath))[0]
    te.loc[0] = [file_start_time, file_stop_time, 'FILE_'+b]
    event_times = event_times.append(te)

    # ff = (exptevents['Trial'] == 3){}
    # exptevents.loc[ff]

    stim_dict = {}

    if 'pertrial' in options and options['pertrial']:
        # NOT COMPLETE!

        # make stimulus events unique to each trial
        this_event_times = event_times.copy()
        for eventidx in range(0, TrialCount):
            event_name = "TRIAL{0}".format(eventidx)
            this_event_times.loc[eventidx, 'name'] = event_name
            if options['stim']:
                stim_dict[event_name] = stim[:, :, eventidx]
        event_times = pd.concat([event_times, this_event_times])

    else:
        # generate stimulus events unique to each distinct stimulus
        ff_tar_events = exptevents['name'].str.endswith('Target')
        ff_tar_pre = exptevents['name'].str.startswith('Pre') & ff_tar_events
        ff_tar_dur = exptevents['name'].str.startswith('Stim') & ff_tar_events
        ff_lick_dur = (exptevents['name'] == 'LICK')
        ff_tar_post = exptevents['name'].str.startswith('Post') & ff_tar_events

        ff_pre_all = exptevents['name'] == ""
        ff_post_all = ff_pre_all.copy()

        for eventidx in range(0, len(tags)):

            if options['stim']:
                # save stimulus for this event as separate dictionary entry
                stim_dict["STIM_" + tags[eventidx]] = stim[:, :, eventidx]
            else:
                stim_dict["STIM_" + tags[eventidx]] = np.array([[]])
            # complicated experiment-specific part
            tag_mask_start = (
                    "PreStimSilence , " + tags[eventidx] + " , Reference"
                    )
            tag_mask_stop = (
                    "PostStimSilence , " + tags[eventidx] + " , Reference"
                    )

            ffstart = (exptevents['name'] == tag_mask_start)
            if np.sum(ffstart) > 0:
                ffstop = (exptevents['name'] == tag_mask_stop)
            else:
                ffstart = (exptevents['name'].str.contains(tag_mask_start))
                ffstop = (exptevents['name'].str.contains(tag_mask_stop))

            # create intial list of stimulus events
            this_event_times = pd.concat(
                    [exptevents.loc[ffstart, ['start']].reset_index(),
                     exptevents.loc[ffstop, ['end']].reset_index()],
                    axis=1
                    )
            this_event_times = this_event_times.drop(columns=['index'])
            this_event_times['name'] = "STIM_" + tags[eventidx]

            # screen for conflicts with target events
            keepevents = np.ones(len(this_event_times)) == 1
            keeppostevents = np.ones(len(this_event_times)) == 1
            for i, d in this_event_times.iterrows():
                fdur = ((ff_tar_dur | ff_lick_dur)
                        & (exptevents['start'] < d['end'] - 0.001)
                        & (exptevents['end'] > d['start'] + 0.001))

                if np.sum(fdur) and \
                   (exptevents['start'][fdur].min() < d['start'] + 0.5):
                    # assume fully overlapping, delete automaticlly
                    # print("Stim (event {0}: {1:.2f}-{2:.2f} {3}"
                    #       .format(eventidx,d['start'], d['end'],d['name']))
                    # print("??? But did it happen?"
                    #       "? Conflicting target: {0}-{1} {2}"
                    #       .format(exptevents['start'][j],
                    #               exptevents['end'][j],
                    #               exptevents['name'][j]))
                    keepevents[i] = False
                    keeppostevents[i] = False
                elif np.sum(fdur):
                    # truncate reference period
                    # print("adjusting {0}-{1}={2}".format(this_event_times['end'][i],
                    #        exptevents['start'][fdur].min(),
                    #        this_event_times['end'][i]-exptevents['start'][fdur].min()))
                    this_event_times.loc[i, 'end'] = \
                       exptevents['start'][fdur].min()
                    keeppostevents[i] = False

            if np.sum(keepevents == False):
                print("Removed {0}/{1} events that overlap with target"
                      .format(np.sum(keepevents == False), len(keepevents)))

            # create final list of these stimulus events
            this_event_times = this_event_times[keepevents]
            tff, = np.where(ffstart)
            ffstart[tff[keepevents == False]] = False
            tff, = np.where(ffstop)
            ffstop[tff[keeppostevents == False]] = False

            event_times = event_times.append(this_event_times,
                                             ignore_index=True)
            this_event_times['name'] = "REFERENCE"
            event_times = event_times.append(this_event_times,
                                             ignore_index=True)
            # event_times = pd.concat([event_times, this_event_times])

            ff_pre_all = ff_pre_all | ffstart
            ff_post_all = ff_post_all | ffstop

        # generate list of corresponding pre/post events
        this_event_times2 = pd.concat(
                [exptevents.loc[ff_pre_all, ['start']],
                 exptevents.loc[ff_pre_all, ['end']]],
                axis=1
                )
        this_event_times2['name'] = 'PreStimSilence'
        this_event_times3 = pd.concat(
                [exptevents.loc[ff_post_all, ['start']],
                 exptevents.loc[ff_post_all, ['end']]],
                axis=1
                )
        this_event_times3['name'] = 'PostStimSilence'

        event_times = event_times.append(this_event_times2, ignore_index=True)
        event_times = event_times.append(this_event_times3, ignore_index=True)

        # create list of target events
        this_event_times = pd.concat(
                [exptevents.loc[ff_tar_pre, ['start']].reset_index(),
                 exptevents.loc[ff_tar_post, ['end']].reset_index()],
                axis=1
                )
        this_event_times = this_event_times.drop(columns=['index'])
        this_event_times['name'] = "TARGET"
        event_times = event_times.append(this_event_times, ignore_index=True)

        for i,e in exptevents[ff_tar_events | ff_lick_dur].iterrows():
            name = e['name']
            elements = name.split(" , ")

            if elements[0] == "PreStimSilence":
                name="PreStimSilence"
            elif elements[0] == "Stim":
                name="TAR_" + elements[1]
                e['start'] = exptevents.loc[i-1]['start']
                e['end'] = exptevents.loc[i+1]['end']
            elif elements[0] == "PostStimSilence":
                name="PostStimSilence"
            else:
                name = "LICK"
                licklen = 0.1
                e['end']=e['start'] + licklen
            print('adding {} {}-{}'.format(name,e['start'], e['end']))
            te = pd.DataFrame(index=[0], columns=(event_times.columns),
                              data=[[e['start'], e['end'], name]])
            event_times = event_times.append(te, ignore_index=True)

        # event_times = pd.concat(
        #         [event_times, this_event_times2, this_event_times3]
        #         )

    # add behavior events
    if exptparams['runclass'] == 'PTD' and any_behavior:
        # special events for tone in noise task
        tar_idx_freq = exptparams['TrialObject'][1]['TargetIdxFreq']
        tar_snr = exptparams['TrialObject'][1]['RelativeTarRefdB']
        common_tar_idx, = np.where(tar_idx_freq == np.max(tar_idx_freq))

        if (isinstance(tar_idx_freq, (int))
                or len(tar_idx_freq) == 1
                or np.isinf(tar_snr[0])):
            diff_event = 'PURETONE_BEHAVIOR'
        elif np.isfinite(tar_snr[0]) & (np.max(common_tar_idx) < 2):
            diff_event = 'EASY_BEHAVIOR'
        elif np.isfinite(tar_snr[0]) & (2 in common_tar_idx):
            diff_event = 'MEDIUM_BEHAVIOR'
        elif np.isfinite(tar_snr[0]) & (np.min(common_tar_idx) > 2):
            diff_event = 'HARD_BEHAVIOR'
        else:
            diff_event = 'PURETONE_BEHAVIOR'
        te = pd.DataFrame(index=[0], columns=(event_times.columns))
        te.loc[0] = [file_start_time, file_stop_time, diff_event]
        event_times = event_times.append(te, ignore_index=True)
        # event_times=pd.concat([event_times, te])

    # sort by when the event occured in experiment time
    event_times = event_times.sort_values(
            by=['start', 'end'], ascending=[1, 0]
            ).reset_index()
    event_times = event_times.drop(columns=['index'])

    return event_times, spike_dict, stim_dict, state_dict


def baphy_load_dataset_RDT(parmfilepath, **options):
    """
    this can be used to generate a recording object for an RDT experiment
        based largely on baphy_load_recording but with several additional
        specialized outputs

    input:
        parmfilepath: baphy parameter file
        options: dictionary of loading options

    current outputs:
        event_times: pandas dataframe with one row per event. times in sec
              since experiment began
        spike_dict: dictionary of lists. spike_dict[cellid] is the set of
              spike times (secs since expt started) for that unit
        stim_dict: stim_dict[name] is [channel X time] stimulus
              (spectrogram) matrix, the times that the stimuli were played
              are rows in the event_times dataframe
        stim1_dict: same thing but for foreground stream only
        stim2_dict: background stream
        state_dict: dictionary of continuous Tx1 signals indicating
           state_dict['repeating_phase']=when in repeating phase
           state_dict['single_stream']=when trial is single stream
           state_dict['targetid']=target id on the current trial

    TODO : merge back into general loading function ? Or keep separate?
    """

    options['pupil'] = options.get('pupil', False)
    options['stim'] = options.get('stim', True)
    options['runclass'] = options.get('runclass', None)

    # get the relatively un-pre-processed data
    exptevents, stim, spike_dict, state_dict, tags, stimparam, exptparams = \
        baphy_load_data(parmfilepath, **options)

    # pre-process event list (event_times) to only contain useful events

    # extract each trial
    tag_mask_start = "TRIALSTART"
    tag_mask_stop = "TRIALSTOP"
    ffstart = exptevents['name'].str.startswith(tag_mask_start)
    ffstop = (exptevents['name'] == tag_mask_stop)
    TrialCount = np.max(exptevents.loc[ffstart, 'Trial'])
    event_times = pd.concat(
            [exptevents.loc[ffstart, ['start']].reset_index(),
             exptevents.loc[ffstop, ['end']].reset_index()],
            axis=1
            )
    event_times['name'] = "TRIAL"
    event_times = event_times.drop(columns=['index'])

    stim_dict = {}
    stim1_dict = {}
    stim2_dict = {}
    state_dict = {}

    # make stimulus events unique to each trial
    this_event_times = event_times.copy()
    rasterfs = options['rasterfs']
    BigStimMatrix = stimparam[-1]
    state = np.zeros([3, stim.shape[1], stim.shape[2]])
    single_stream_trials = (BigStimMatrix[0, 1, :] == -1)
    state[1, :, single_stream_trials] = 1
    prebins = int(exptparams['TrialObject'][1]['PreTrialSilence']*rasterfs)
    samplebins = int(
            exptparams['TrialObject'][1]['ReferenceHandle'][1]['Duration']
            * rasterfs
            )

    for trialidx in range(0, TrialCount):
        event_name = "TRIAL{0}".format(trialidx)
        this_event_times.loc[trialidx, 'name'] = event_name
        stim1_dict[event_name] = stim[:, :, trialidx, 0]
        stim2_dict[event_name] = stim[:, :, trialidx, 1]
        stim_dict[event_name] = stim[:, :, trialidx, 2]

        s = np.zeros([3, stim_dict[event_name].shape[1]])
        rslot = np.argmax(np.diff(BigStimMatrix[:, 0, trialidx]) == 0) + 1
        rbin = prebins + rslot*samplebins
        s[0, rbin:] = 1
        single_stream_trial = int(BigStimMatrix[0, 1, trialidx] == -1)
        s[1, :] = single_stream_trial
        tarslot = np.argmin(BigStimMatrix[:, 0, trialidx] > 0) - 1
        s[2, :] = BigStimMatrix[tarslot, 0, trialidx]
        state_dict[event_name] = s

    state_dict['BigStimMatrix'] = BigStimMatrix

    event_times = pd.concat([event_times, this_event_times])

    # add stim events
    stim_mask = "Stim ,"
    ffstim = (exptevents['name'].str.contains(stim_mask))
    stim_event_times = exptevents.loc[ffstim, ['name', 'start', 'end']]
    event_times = pd.concat([event_times, stim_event_times])

    # sort by when the event occured in experiment time
    event_times = event_times.sort_values(by=['start', 'end'])
    cols = ['name', 'start', 'end']
    event_times = event_times[cols]
#    for trialidx in range(0,TrialCount):
#       rslot=np.argmax(np.diff(BigStimMatrix[:,0,trialidx])==0)+1
#       rbin=prebins+rslot*samplebins
#       state[0,rbin:,trialidx]=1
#
#       tarslot=np.argmin(BigStimMatrix[:,0,trialidx]>0)-1
#       state[2,:,trialidx]=BigStimMatrix[tarslot,0,trialidx]
#
#    state_dict['repeating_phase']=np.reshape(state[0,:,:].T,[-1,1])
#    state_dict['single_stream']=np.reshape(state[0,:,:].T,[-1,1])
#    state_dict['targetid']=np.reshape(state[0,:,:].T,[-1,1])

    return (event_times, spike_dict, stim_dict, state_dict,
            stim1_dict, stim2_dict)


def spike_time_to_raster(spike_dict, fs=100, event_times=None):
    """
    convert list of spike times to a raster of spike rate, with duration
    matching max end time in the event_times list
    """

    if event_times is not None:
        maxtime = np.max(event_times["end"])

    maxbin = np.int(np.ceil(fs*maxtime))
    unitcount = len(spike_dict.keys())
    raster = np.zeros([unitcount, maxbin])

    cellids = sorted(spike_dict)
    for i, key in enumerate(cellids):
        for t in spike_dict[key]:
            b = int(np.floor(t*fs))
            if b < maxbin:
                raster[i, b] += 1

    return raster, cellids


def dict_to_signal(stim_dict, fs=100, event_times=None, signal_name='stim',
                   recording_name='rec'):

    maxtime = np.max(event_times["end"])
    maxbin = int(fs*maxtime)

    tags = list(stim_dict.keys())
    chancount = stim_dict[tags[0]].shape[0]

    z = np.zeros([chancount, maxbin])

    empty_stim = nems.signal.RasterizedSignal(
            data=z, fs=fs, name=signal_name,
            epochs=event_times, recording=recording_name
            )
    stim = empty_stim.replace_epochs(stim_dict)
    #stim = stim.normalize('minmax')

    return stim


def baphy_load_recording(cellid, batch, **options):
    """
    query NarfData to find baphy files for specified cell/batch and then load
    """

    # print(options)
    options['rasterfs'] = int(options.get('rasterfs', 100))
    options['stimfmt'] = options.get('stimfmt', 'ozgf')
    options['chancount'] = int(options.get('chancount', 18))
    options['pertrial'] = int(options.get('pertrial', False))
    options['includeprestim'] = options.get('includeprestim', 1)

    options['pupil'] = int(options.get('pupil', False))
    options['pupil_deblink'] = int(options.get('pupil_deblink', 1))
    options['pupil_median'] = int(options.get('pupil_median', 1))
    options['stim'] = int(options.get('stim', True))
    options['runclass'] = options.get('runclass', None)
    options['cellid'] = options.get('cellid', cellid)
    options['batch'] = int(batch)
    options['rawid'] = options.get('rawid', None)

    d = db.get_batch_cell_data(batch=batch,
                               cellid=cellid,
                               rawid=options['rawid'],
                               label='parm')
    if len(d)==0:
        raise ValueError('cellid/batch entry not found in NarfData')

    files = list(d['parm'])

    for i, parmfilepath in enumerate(files):

        if options["runclass"] == "RDT":
            event_times, spike_dict, stim_dict, \
                state_dict, stim1_dict, stim2_dict = \
                baphy_load_dataset_RDT(parmfilepath, **options)
        else:
            event_times, spike_dict, stim_dict, state_dict = \
                baphy_load_dataset(parmfilepath, **options)

            d2 = event_times.loc[0].copy()
            if (i == 0) and (d2['name'] == 'PASSIVE_EXPERIMENT'):
                d2['name'] = 'PRE_PASSIVE'
                event_times = event_times.append(d2)
            elif d2['name'] == 'PASSIVE_EXPERIMENT':
                d2['name'] = 'POST_PASSIVE'
                event_times = event_times.append(d2)

        # generate spike raster
        raster_all, cellids = spike_time_to_raster(
                spike_dict, fs=options['rasterfs'], event_times=event_times
                )

        # generate response signal
        t_resp = nems.signal.RasterizedSignal(
                fs=options['rasterfs'], data=raster_all, name='resp',
                recording=cellid, chans=cellids, epochs=event_times
                )
        if i == 0:
            resp = t_resp
        else:
            resp = resp.concatenate_time([resp, t_resp])

        if options['pupil']:

            # create pupil signal if it exists
            rlen = int(t_resp.ntimes)
            pcount = state_dict['pupiltrace'].shape[0]
            plen = state_dict['pupiltrace'].shape[1]
            if plen > rlen:
                state_dict['pupiltrace'] = \
                    state_dict['pupiltrace'][:, 0:-(plen-rlen)]
            elif rlen > plen:
                state_dict['pupiltrace']=np.append(state_dict['pupiltrace'],
                          np.ones([pcount,rlen-plen])*np.nan,
                          axis=1)

            # generate pupil signals
            t_pupil = nems.signal.RasterizedSignal(
                    fs=options['rasterfs'], data=state_dict['pupiltrace'],
                    name='pupil', recording=cellid, chans=['pupil'],
                    epochs=event_times
                    )

            if i == 0:
                pupil = t_pupil
            else:
                pupil = pupil.concatenate_time([pupil, t_pupil])

        if options['stim']:
            t_stim = dict_to_signal(stim_dict, fs=options['rasterfs'],
                                    event_times=event_times)
            t_stim.recording = cellid

            if i == 0:
                print("i={0} starting".format(i))
                stim = t_stim
            else:
                print("i={0} concatenating".format(i))
                stim = stim.concatenate_time([stim, t_stim])

        if options['stim'] and options["runclass"] == "RDT":
            t_BigStimMatrix = state_dict['BigStimMatrix']
            del state_dict['BigStimMatrix']

            t_stim1 = dict_to_signal(
                    stim1_dict, fs=options['rasterfs'],
                    event_times=event_times, signal_name='stim1',
                    recording_name=cellid
                    )
            t_stim2 = dict_to_signal(
                    stim2_dict, fs=options['rasterfs'],
                    event_times=event_times, signal_name='stim2',
                    recording_name=cellid
                    )
            t_state = dict_to_signal(
                    state_dict, fs=options['rasterfs'],
                    event_times=event_times, signal_name='state',
                    recording_name=cellid
                    )
            t_state.chans = ['repeating_phase', 'single_stream', 'targetid']

            if i == 0:
                print("i={0} starting".format(i))
                stim1 = t_stim1
                stim2 = t_stim2
                state = t_state
                BigStimMatrix = t_BigStimMatrix
            else:
                print("i={0} concatenating".format(i))
                stim1 = stim1.concatenate_time([stim1, t_stim1])
                stim2 = stim2.concatenate_time([stim2, t_stim2])
                state = state.concatenate_time([state, t_state])
                BigStimMatrix = np.concatenate(
                        (BigStimMatrix, t_BigStimMatrix), axis=2
                        )

    resp.meta = options

    signals = {'resp': resp}

    if options['pupil']:
        signals['pupil'] = pupil
    if options['stim']:
        signals['stim'] = stim

    if options['stim'] and (options["runclass"] == "RDT"):
        signals['stim1'] = stim1
        signals['stim2'] = stim2
    if options["runclass"] == "RDT":
        signals['state'] = state
        # signals['stim'].meta = {'BigStimMatrix': BigStimMatrix}

    rec = nems.recording.Recording(signals=signals)
    return rec


def baphy_load_recording_nonrasterized(**options):
    """
    Input options must include
        batch: integer
        cellid or cell_list or siteid:
            cellid = string, name of single cell
            cell_list = list of cellids
            siteid = siteid, will load all cells at that site

    Returns
        rec: recording

    """
    cellid = options.get('cellid', None)
    batch = options.get('batch', None)
    cell_list = options.get('cell_list', None)
    siteid = options.get('siteid', None)

    if (cellid is None) and (cell_list is None) and (siteid is None):
        raise ValueError("must provide cellid, cell_list or siteid")
    if batch is None:
        raise ValueError("must provide batch")

    if type(cellid) is list:
        cell_list = cellid

    if cell_list is not None:
        cellid = cell_list[0]
        siteid = cellid.split("-")[0]
        rec_name = siteid
        options['cellid'] = cell_list

    elif siteid is not None:
        celldata = db.get_batch_cells(batch=batch, cellid=siteid)
        cell_list = list(celldata['cellid'])
        cellid = cell_list[0]
        rec_name = siteid
        options['cellid'] = cell_list

    elif cellid is not None:
        siteid = cellid.split("-")[0]
        if siteid==cellid:
            celldata = db.get_batch_cells(batch=batch, cellid=siteid)
            cell_list = list(celldata['cellid'])
            cellid = cell_list[0]
            rec_name = siteid
            options['cellid'] = cell_list
        else:
            rec_name = cellid

    else:
        raise ValueError("Does this ever execute?")
        celldata = db.get_batch_cells(batch=batch, cellid=siteid)
        cell_list = list(celldata['cellid'])
        cellid = cell_list[0]
        rec_name = siteid
        options['cellid'] = cell_list

    # set default options if missing
    options['rasterfs'] = int(options.get('rasterfs', 100))
    options['stimfmt'] = options.get('stimfmt', 'ozgf')
    options['chancount'] = int(options.get('chancount', 18))
    options['pertrial'] = int(options.get('pertrial', False))
    options['includeprestim'] = options.get('includeprestim', 1)
    options['pupil'] = int(options.get('pupil', False))
    options['pupil_deblink'] = int(options.get('pupil_deblink', 1))
    options['pupil_median'] = int(options.get('pupil_median', 1))
    options['stim'] = int(options.get('stim', True))
    options['runclass'] = options.get('runclass', None)
    options['cellid'] = options.get('cellid', cellid)
    options['batch'] = int(batch)
    options['rawid'] = options.get('rawid', None)

    # query database to find all baphy files that belong to this cell/batch
    d = db.get_batch_cell_data(batch=batch, cellid=cellid, label='parm',
                               rawid=options['rawid'])

    files = list(set(list(d['parm'])))
    if len(files) == 0:
       raise ValueError('NarfData not found for cell {0}/batch {1}'.format(cellid,batch))

    for i, parmfilepath in enumerate(files):
        # load the file and do a bunch of preprocessing:
        if options["runclass"] == "RDT":
            event_times, spike_dict, stim_dict, \
                state_dict, stim1_dict, stim2_dict = \
                baphy_load_dataset_RDT(parmfilepath, **options)
        else:
            event_times, spike_dict, stim_dict, state_dict = \
                baphy_load_dataset(parmfilepath, **options)

            d2 = event_times.loc[0].copy()
            if (i == 0) and (d2['name'] == 'PASSIVE_EXPERIMENT'):
                d2['name'] = 'PRE_PASSIVE'
                event_times = event_times.append(d2)
            elif d2['name'] == 'PASSIVE_EXPERIMENT':
                d2['name'] = 'POST_PASSIVE'
                event_times = event_times.append(d2)

        # OLD generate spike raster - don't need to do this b/c we're saving
        # spike times now.
        # raster_all, cellids = spike_time_to_raster(
        #       spike_dict,fs=options['rasterfs'],event_times=event_times)

        # generate response signal
        t_resp = nems.signal.PointProcess(
                fs=options['rasterfs'], data=spike_dict,
                name='resp', recording=rec_name, chans=list(spike_dict.keys()),
                epochs=event_times
                )

        if i == 0:
            resp = t_resp
        else:
            # concatenate onto end of main response signal
            resp = resp.append_time(t_resp)

        if options['pupil']:

            # create pupil signal if it exists
            rlen = int(t_resp.ntimes)
            pcount = state_dict['pupiltrace'].shape[0]
            plen = state_dict['pupiltrace'].shape[1]
            if plen > rlen:
                state_dict['pupiltrace'] = \
                    state_dict['pupiltrace'][:, 0:-(plen-rlen)]
            elif rlen > plen:
                state_dict['pupiltrace'] = \
                    np.append(state_dict['pupiltrace'],
                              np.ones([pcount, rlen - plen]) * np.nan,
                              axis=1)

            # generate pupil signals
            t_pupil = nems.signal.RasterizedSignal(
                    fs=options['rasterfs'], data=state_dict['pupiltrace'],
                    name='pupil', recording=rec_name, chans=['pupil'],
                    epochs=event_times)

            if i == 0:
                pupil = t_pupil
            else:
                pupil = pupil.concatenate_time([pupil, t_pupil])

        if options['stim']:
            # accumulate dictionaries
            # CRH replaced cellid w/ site (for when cellid is list)
            t_stim = nems.signal.TiledSignal(
                    data=stim_dict, fs=options['rasterfs'], name='stim',
                    epochs=event_times, recording=rec_name
                    )

            if i == 0:
                print("i={0} starting".format(i))
                stim = t_stim
            else:
                print("i={0} concatenating".format(i))
                # TODO implement concatenate_time for SignalDictionary
                # this basicall just needs to merge the data dictionaries
                # a la : new_dict={**stim._data,**t_stim.data}
                stim = stim.append_time(t_stim)

        if options['stim'] and options["runclass"] == "RDT":
            raise ValueError("RDT not supported yet")

    resp.meta = options

    signals = {'resp': resp}

    if options['pupil']:
        signals['pupil'] = pupil
    if options['stim']:
        signals['stim'] = stim

    # TODO: stim1 and state no longer defined?
    #       just don't support RDT right now?
    if options['stim'] and options["runclass"] == "RDT":
        signals['stim1'] = stim1
        signals['stim2'] = stim2
    if options["runclass"] == "RDT":
        signals['state'] = state
        # signals['stim'].meta={'BigStimMatrix': BigStimMatrix}

    rec = nems.recording.Recording(signals=signals)
    return rec


def baphy_data_path(**options):
    """
    required entries in options dictionary:
        cellid: string or list
            string can be a valid cellid or siteid
            list is a list of cellids from a single(?) site
        batch: integer
        rasterfs
        stimfmt
        chancount
    """

    options['recache'] = options.get('recache', 0)

    # three ways to select cells
    cellid = options.get('cellid', None)
    if type(cellid) is list:
        cellid = cellid[0].split("-")[0]

    elif cellid is None and options.get('siteid') is not None:
        cellid = options.get('siteid')

    if options['stimfmt'] in ['envelope', 'parm']:
        log.info("Setting chancount=0 for stimfmt=%s", options['stimfmt'])
        options['chancount'] = 0

    if (options.get('cellid') is None) or \
       (options.get('batch') is None) or \
       (options.get('rasterfs') is None) or \
       (options.get('stimfmt') is None) or \
       (options.get('chancount') is None):
        raise ValueError("cellid,batch,rasterfs,stimfmt,chancount options required")

    # TODO : base filename on siteid/cellid plus hash from JSON-ized options
    data_path = ("/auto/data/nems_db/recordings/{0}/{1}{2}_fs{3}/"
                 .format(options["batch"], options['stimfmt'],
                         options["chancount"], options["rasterfs"]))
    data_file = data_path + cellid + '.tgz'

    log.info(data_file)
    log.info(options)

    if not os.path.exists(data_file) or options['recache'] is True:
        #  rec = baphy_load_recording(
        #          options['cellid'], options['batch'], options
        #          )
        rec = baphy_load_recording_nonrasterized(**options)
        print(rec.name)
        rec.save(data_file)

    return data_file


def baphy_load_multichannel_recording(**options):
    """
    TESTING - CRH 6/29/2018
    Meant to function as a wrapper around baphy_data_path. Will find all cellids
    matching the batch and site specified (and rawids if given). Then, will
    load all cells for this batch/rawids at this site, then
    cache it and return the recording uri.
    The cache will also save a json file containing all the options information
    for the recording. If the recording has been loaded before with the same
    cellids and options, it will just be re-loaded from cache.
    IMPORTANT, we include the check for cellids because it's possible that data
    could get resorted and the same batch/site/options might no longer mean the
    same cellids!
    """

    try:
        batch = options.get('batch')
        site = options.get('site')
        if site is None:
            site = options.get('siteid')
    except ValueError:
        raise ValueError("must provide site and batch parameters")

    options['rasterfs'] = int(options.get('rasterfs', 100))
    options['stimfmt'] = options.get('stimfmt', 'ozgf')
    options['chancount'] = int(options.get('chancount', 18))
    options['pertrial'] = int(options.get('pertrial', False))
    options['includeprestim'] = options.get('includeprestim', 1)

    options['pupil'] = int(options.get('pupil', False))
    options['pupil_deblink'] = int(options.get('pupil_deblink', 1))
    options['pupil_median'] = int(options.get('pupil_median', 1))
    options['stim'] = int(options.get('stim', False))
    options['runclass'] = options.get('runclass', None)
    options['recache'] = options.get('recache', False)

    # TODO - this really should be smarter - pad with Nans or something...
    # For the time being...
    # If rawids are not specificed, will only load cellids that are stable across
    # all rawids matching this batch and site.
    if options.get('rawid') is None:
        # Query database to get the matching cellids and rawids
        cellids, all_rawids = db.get_stable_batch_cellids(batch=batch,
                                                          cellid=site)
        options['rawid'] = options.get('rawid', all_rawids)
    else:
        cellids, _ = db.get_stable_batch_cellids(batch=batch,
                                                 cellid=site,
                                                 rawid=options['rawid'])
    if type(cellids[0]) is np.ndarray:
        cellids = list(cellids[0])  # full list of cellids stored
    elif type(cellids[0]) is str and type(cellids) is np.ndarray:
        cellids = list(cellids)
    elif type(cellids) is list:
        cellids = cellids
    else:
        raise ValueError("what's going on?")

    unique_id = str(datetime.datetime.now()).split('.')[0].replace(' ', '-')
    full_rec_uri = '/auto/users/hellerc/recordings/'+str(batch)+'/'+site+'_'+unique_id+'.tgz'
    full_rec_meta = full_rec_uri.split('.')[0:-1][0]+'.json'

    # since all uri's will have a unique id, we don't check for the exact name existing, we look
    # for something with the same batch/site and identical meta data (load options)
    search_str = full_rec_meta.split('_')[0]
    meta_data_files = glob.glob(search_str+'*'+'.json')
    cache_exists=None

    options['cellid'] = cellids
    t_options = options.copy()
    # t_options just so that recache field doesn't mess up caching system check (CRH)
    if 'recache' in t_options:
        del t_options['recache']
    for mdf in meta_data_files:
        with open(mdf,'r') as fp:
            x = json.load(fp)
        if x == t_options:
            if options['recache'] == True:
                print('Found cached recording with given options, deleting and regenerating...')
                os.remove(mdf)
                os.remove(mdf.split('.')[0:-1][0]+'.tgz')
            else:
                full_rec_meta = mdf
                full_rec_uri = mdf.split('.')[0:-1][0]+'.tgz'
                print('Found cached recording, returning {0}'.format(full_rec_uri))
                cache_exists = True
                continue

    if cache_exists is None or options['recache'] == True:

        print('No cache recording found. Calling baphy_data path to generate new rec')
        rec_uri = baphy_data_path(**options)
        rec = Recording.load(rec_uri)

        # rec['resp']  = rec['resp'].rasterize()

        rec.save(full_rec_uri)

        with open(full_rec_meta,'w') as fp:
            json.dump(t_options, fp)

        return full_rec_uri

    else:
        return full_rec_uri


def load_recordings(recording_uri_list, cellid, **context):
    """
    cellid can be single cell, or list of cells. Whatever it is, the cellids
    must exist in the resp channels of the recordings that are being loaded.

    crh - testing this for use w/ xforms... 7/11/2018
    """

    rec = load_recording(recording_uri_list[0])
    other_recordings = [load_recording(uri) for uri in recording_uri_list[1:]]
    if other_recordings:
        rec.concatenate_recordings(other_recordings)

    if not isinstance(cellid, list):
        cellid = [cellid]

    # check to see if only a siteid was passed. If this is the case, load entire
    # recording
    if re.search(r'\d+$', cellid[0]) is None:
        print('loading all cellids at site')
    else:
        print('extracting channels: {0}'.format(cellid))
        r = rec['resp'].extract_channels(cellid)
        rec.add_signal(r)

    if 'pupil' in rec.signals.keys() and np.any(np.isnan(rec['pupil'].as_continuous())):
                log.info('Padding {0} with the last non-nan value'.format('pupil'))
                inds = ~np.isfinite(rec['pupil'].as_continuous())
                arr = copy.deepcopy(rec['pupil'].as_continuous())
                arr[inds] = arr[~inds][-1]
                rec['pupil'] = rec['pupil']._modified_copy(arr)

    return {'rec': rec}


def get_kilosort_template(batch=None, cellid=None):
    """
    return the waveform template for the given cellid. only works for cellids
    with the current naming scheme i.e. TAR017b-07-2. crh 2018-07-24
    """
    parmfile = db.get_batch_cell_data(batch=batch, cellid=cellid).T.values[0][0]
    path = os.path.dirname(parmfile)+'/sorted/'
    rootname = ('.').join([os.path.basename(parmfile).split('.')[0], 'spk.mat'])
    spkfile = path+rootname
    sortdata = scipy.io.loadmat(spkfile, chars_as_strings=True)

    try:
        chan = int(cellid[-4:-2])
        unit = int(cellid[-1:])
        template = sortdata['sortinfo'][0][chan-1][0][0][unit-1][0]['Template'][chan-1,:]
    except:
        template=np.nan

    return template


def get_kilosort_templates(batch=None):
    """
    Return dataframe containing the waveform template for every cellid in this
    batch. crh 2018-07-24
    """
    cellids = db.get_batch_cells(batch)['cellid']
    df = pd.DataFrame(index=cellids, columns=['template'])

    for c in cellids:
        df.loc[c]['template'] = get_kilosort_template(batch=batch, cellid=c)

    return df
